{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"audioclass","text":"<p>A Python library that simplifies the process of using machine learning models for audio classification.</p>"},{"location":"#description","title":"Description","text":"<p>Audioclass provides a unified interface for working with various audio classification models, making it easier to load, preprocess, batch process, and analyze audio data. It offers:</p> <ul> <li>Standardized Model Interface: Easily swap between different model implementations (TensorFlow, TensorFlow Lite, etc.) without changing your code.</li> <li>Flexible Data Loading: Load audio data from files, directories, or pandas DataFrames with a few simple commands.</li> <li>Efficient Batch Processing: Effortlessly process large datasets in batches for faster inference.</li> <li>Unified Postprocessing: Convert model outputs into easy-to-use formats like xarray datasets or soundevent objects.</li> <li>Pre-trained Models: Get started quickly with built-in support for popular models like BirdNET and Perch.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>Get started with audioclass in a snap:</p> <pre><code>pip install audioclass\n</code></pre> <p>Optional Dependencies:</p> <ul> <li>For BirdNET: Install additional dependencies using <code>pip install \"audioclass[birdnet]\"</code>.</li> <li>For Perch: Install additional dependencies using <code>pip install \"audioclass[perch]\"</code>.</li> </ul>"},{"location":"#how_to_use_it","title":"How to Use It","text":"<p>Here's a quick example of how to load the BirdNET model, preprocess an audio file, and get predictions:</p> <pre><code>from audioclass.models.birdnet import BirdNET\n\n# Load the model\nmodel = BirdNET.load()\n\n# Get predictions\npredictions = model.process_file(\"path/to/audio/file.wav\")\n\nprint(predictions)\n</code></pre> <p>For more detailed examples, tutorials, and complete API documentation, visit our documentation website.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>We welcome contributions to audioclass! If you'd like to get involved, please check out our Contributing Guidelines.</p>"},{"location":"#attribution","title":"Attribution","text":"<ul> <li> <p>The BirdNET model was developed by the K. Lisa Yang Center for Conservation Bioacoustics at the Cornell Lab of Ornithology, in collaboration with Chemnitz University of Technology. This package is not affiliated with the BirdNET project. If you use the BirdNET model, please cite the relevant paper (see the <code>audioclass.models.birdnet</code> module docstring for details).</p> </li> <li> <p>The Perch model is a research product by Google Research. This package is not affiliated with Google Research.</p> </li> </ul> <p>Audioclass is an independent project and is not affiliated with either the BirdNET or Perch projects.</p>"},{"location":"CODE_OF_CONDUCT/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"CODE_OF_CONDUCT/#our_pledge","title":"Our Pledge","text":"<p>In the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, religion, or sexual identity and orientation.</p>"},{"location":"CODE_OF_CONDUCT/#our_standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to creating a positive environment include:</p> <ul> <li>Using welcoming and inclusive language</li> <li>Being respectful of differing viewpoints and experiences</li> <li>Gracefully accepting constructive criticism</li> <li>Focusing on what is best for the community</li> <li>Showing empathy towards other community members</li> </ul> <p>Examples of unacceptable behavior by participants include:</p> <ul> <li>The use of sexualized language or imagery and unwelcome sexual attention or   advances</li> <li>Trolling, insulting/derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or electronic   address, without explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"CODE_OF_CONDUCT/#our_responsibilities","title":"Our Responsibilities","text":"<p>Project maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.</p> <p>Project maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.</p>"},{"location":"CODE_OF_CONDUCT/#scope","title":"Scope","text":"<p>This Code of Conduct applies both within project spaces and in public spaces when an individual is representing the project or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed representative at an online or offline event. Representation of a project may be further defined and clarified by project maintainers.</p>"},{"location":"CODE_OF_CONDUCT/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://www.contributor-covenant.org/version/1/4/code-of-conduct.html</p>"},{"location":"CONTRIBUTING/","title":"Contributing guidelines","text":"<p>We welcome any kind of contribution to <code>audioclass</code>, from simple comment or question to a full fledged pull request. Please read and follow our Code of Conduct.</p> <p>A contribution can be one of the following cases:</p> <ol> <li>you have a question;</li> <li>you think you may have found a bug (including unexpected behavior);</li> <li>you want to make some kind of change to the code base (e.g. to fix a bug, to add a new feature, to update documentation).</li> </ol> <p>The sections below outline the steps in each case.</p>"},{"location":"CONTRIBUTING/#you_have_a_question","title":"You have a question","text":"<ol> <li>use the search functionality here to see if someone already filed the same issue;</li> <li>if your issue search did not yield any relevant results, make a new issue;</li> <li>apply the \"Question\" label; apply other labels when relevant.</li> </ol>"},{"location":"CONTRIBUTING/#you_think_you_may_have_found_a_bug","title":"You think you may have found a bug","text":"<ol> <li>use the search functionality here to see if someone already filed the same issue;</li> <li>if your issue search did not yield any relevant results, make a new issue, making sure to provide enough information to the rest of the community to understand the cause and context of the problem. Depending on the issue, you may want to include:</li> <li>the SHA hashcode of the commit that is causing your problem;</li> <li>some identifying information (name and version number) for dependencies you're using;</li> <li>information about the operating system;</li> <li>apply relevant labels to the newly created issue.</li> </ol>"},{"location":"CONTRIBUTING/#you_want_to_make_some_kind_of_change_to_the_code_base","title":"You want to make some kind of change to the code base","text":"<ol> <li>(important) announce your plan to the rest of the community before you start working. This announcement should be in the form of a (new) issue;</li> <li>(important) wait until some kind of consensus is reached about your idea being a good idea;</li> <li>if needed, fork the repository to your own Github profile and create your own feature branch off of the latest main commit. While working on your feature branch, make sure to stay up to date with the main branch by pulling in changes, possibly from the 'upstream' repository (follow the instructions here and here);</li> <li>Install dependencies with <code>pip install -e .</code>;</li> <li>make sure the existing tests still work by running <code>pytest</code>. If project tests fails use <code>pytest --keep-baked-projects</code> to keep generated project in /tmp/pytest-* and investigate;</li> <li>add your own tests (if necessary);</li> <li>update or expand the documentation;</li> <li>push your feature branch to (your fork of) the Python Template repository on GitHub;</li> <li>create the pull request, e.g. following the instructions here.</li> </ol> <p>In case you feel like you've made a valuable contribution, but you don't know how to write or run tests for it, or how to generate the documentation: don't let this discourage you from making the pull request; we can help you! Just go ahead and submit the pull request, but keep in mind that you might be asked to append additional commits to your pull request.</p>"},{"location":"usage_guide/","title":"Usage Guide","text":""},{"location":"usage_guide/#introduction","title":"Introduction","text":"<p>audioclass is a Python library designed to simplify audio classification tasks using machine learning. It provides a standardized interface for working with various audio classification models, regardless of their underlying implementation. audioclass streamlines the process of loading, preprocessing, and analyzing audio data, enabling easy integration with diverse machine learning models and making audio classification more accessible.</p>"},{"location":"usage_guide/#model_list","title":"Model List","text":"<p>audioclass includes built-in support for two popular audio classification models:</p> <ul> <li> <p>BirdNET: A TensorFlow Lite model   specifically designed for bird sound classification. BirdNET can identify a   wide range of bird species and is suitable for biodiversity monitoring and   ecological research.</p> </li> <li> <p>BirdNETAnalyzer: A   TensorFlow-based model designed for bird sound classification, offering   potential GPU acceleration. BirdNETAnalyzer provides a similar functionality to   the existing BirdNET model but is optimized for scenarios where computational   resources are abundant and inference speed is a priority.</p> </li> <li> <p>Perch: A TensorFlow Hub model developed   by Google Research for bird sound classification. Perch is trained on a large   dataset of bird vocalizations and is known for its quality embeddings.</p> </li> </ul>"},{"location":"usage_guide/#using_the_models","title":"Using the Models","text":"<p>Working with audio classification models in <code>audioclass</code> is simple and intuitive. All models share a consistent interface, making it easy to switch between them without significant code changes.</p> <p>You have several options for processing audio data, depending on your starting point. Let's look at how you can process audio data using the BirdNET model:</p> <pre><code>from audioclass.models.birdnet import BirdNET\nfrom soundevent import data\n\nmodel = BirdNET.load()\nrecording = data.Recording.from_file(\"path/to/your/audio.wav\")\npredictions = model.process_recording(recording)\n\n# Explore predictions\nfor clip_prediction in predictions:\n    print(\n        clip_prediction.clip.start_time,\n        clip_prediction.clip.end_time,\n        clip_prediction.tags,\n    )\n</code></pre> <p>In addition to processing whole recordings, <code>audioclass</code> offers flexibility to work with different types of audio data:</p>"},{"location":"usage_guide/#process_a_file_directly","title":"Process a File Directly","text":"<p>If you don't need the extra information from a Recording object, you can pass the file path directly:</p> <pre><code>predictions = model.process_file(\"path/to/your/audio.wav\")\n</code></pre>"},{"location":"usage_guide/#process_a_clip","title":"Process a Clip","text":"<p>Isolate a specific segment of a recording for analysis:</p> <pre><code>clip = data.Clip(start_time=10, end_time=20, recording=recording)\npredictions = model.process_clip(clip)\n</code></pre>"},{"location":"usage_guide/#process_raw_audio_data","title":"Process Raw Audio Data","text":"<p>For maximum control, work with raw audio arrays (NumPy arrays):</p> <pre><code>import numpy as np\nraw_audio = np.random.randn(44100)  # Random audio (replace with your data)\npredictions = model.process_array(raw_audio)\n</code></pre>"},{"location":"usage_guide/#interchangeable_models","title":"Interchangeable Models:","text":"<p>All models in <code>audioclass</code> adhere to the same interface, making it effortless to switch between them:</p> <pre><code>from audioclass.models.perch import Perch  # Switch to the Perch model\n\nmodel = Perch.load()\n# ... the rest of your code remains the same!\n</code></pre>"},{"location":"usage_guide/#model_attributes","title":"Model Attributes","text":"<p>Each model provides essential information through its attributes:</p> <ul> <li><code>model.samplerate</code>: The sample rate the model expects (e.g., 48000 Hz for BirdNET).</li> <li><code>model.input_samples</code>: The number of samples per audio frame the model requires.</li> <li><code>model.tags</code>: The labels the model can predict.</li> </ul>"},{"location":"usage_guide/#model_outputs","title":"Model Outputs","text":"<p><code>audioclass</code> models provide results as a list of <code>ClipPrediction</code> objects from the [<code>soundevent</code>][] library. These objects offer a convenient, standardized way to access predictions and features for each audio clip processed.</p>"},{"location":"usage_guide/#whats_inside_a_clipprediction","title":"What's Inside a <code>ClipPrediction</code>?","text":"<p>Each <code>ClipPrediction</code> object holds essential information:</p> <ul> <li>Clip Details:</li> <li>The <code>clip</code> attribute provides the audio clip's start and end times within the original recording.</li> <li>The <code>recording</code> attribute gives you access to details about the original recording (e.g., location, date, etc.).</li> <li>Predicted Tags: The <code>tags</code> attribute is a list of <code>PredictedTag</code> objects, each representing a class label the model thinks is present in the clip. Each <code>PredictedTag</code> has:</li> <li>A <code>tag</code> attribute for the class label.</li> <li>A <code>score</code> attribute indicating the model's confidence in that label (typically a probability between 0 and 1).</li> <li>Extracted Features: The <code>features</code> attribute contains a list of <code>Feature</code> objects, representing numerical representations extracted from the audio by the model. These features can be used for further analysis, clustering, or even as input for other machine learning tasks.</li> </ul>"},{"location":"usage_guide/#working_with_clipprediction","title":"Working with <code>ClipPrediction</code>","text":"<pre><code>outputs = model.process_recording(recording)\n\nclip_prediction = outputs[0] # access the 1st prediction\n\n# Get the audio clip information\nclip = clip_prediction.clip\nrecording = clip.recording\nstart_time = clip.start_time\nend_time = clip.end_time\n\n# Access predicted tags\nfor predicted_tag in clip_prediction.tags:\n    tag = predicted_tag.tag\n    score = predicted_tag.score\n    print(f\"The tag '{tag}' was predicted to be present in the clip with {score:.2f} confidence.\")\n\n# Access extracted features\nfor feature in clip_prediction.features:\n    print(f\"Extracted feature: {feature.name} - {feature.value}\")\n</code></pre> <p>As mentioned, the <code>soundevent</code> package gives a convenient way of working with model outputs. For a full description of these objects please refer to the <code>soundevent</code> documentation.</p>"},{"location":"usage_guide/#batch_processing","title":"Batch Processing","text":"<p>Working with large audio datasets? <code>audioclass</code> provides efficient batch processing through specialized iterators. These iterators handle the loading, preprocessing, and batching of your audio recordings, allowing you to focus on analysis.</p>"},{"location":"usage_guide/#simpleiterator_your_starting_point","title":"<code>SimpleIterator</code>: Your Starting Point","text":"<p>The <code>SimpleIterator</code> is a great option for getting started with batch processing:</p> <pre><code>from audioclass.models.birdnet import BirdNET\nfrom audioclass.batch import SimpleIterator\nfrom soundevent import data\n\nmodel = BirdNET.load()\nrecordings = ...  # Load a list of soundevent Recording objects\niterable = SimpleIterator(\n    recordings,\n    samplerate=model.samplerate,\n    input_samples=model.input_samples\n)\npredictions = model.process_iterable(iterable)\n</code></pre>"},{"location":"usage_guide/#more_flexibility_with_data_sources","title":"More Flexibility with Data Sources","text":"<p><code>SimpleIterator</code> can handle more than just lists of <code>Recording</code> objects. You can easily create an iterator from:</p> <ul> <li>A list of file paths:</li> </ul> <pre><code>list_of_files = [\"path/to/file1.wav\", \"path/to/file2.wav\", \"path/to/file3.wav\"]\niterable = SimpleIterator.from_files(\n    list_of_files,\n    samplerate=model.samplerate,\n    input_samples=model.input_samples,\n)\n</code></pre> <ul> <li>An audio directory: (Recursively searches for audio files)</li> </ul> <pre><code>audio_directory = \"path/to/directory\"\niterable = SimpleIterator.from_directory(\n    audio_directory,\n    samplerate=model.samplerate,\n    input_samples=model.input_samples\n)\n</code></pre> <ul> <li>A pandas DataFrame:</li> </ul> <pre><code>import pandas as pd\n\ndf = pd.DataFrame({\"path\": list_of_files, \"latitude\": [0, 1, 2], \"longitude\": [3, 4, 5]})  # Example DataFrame\niterable = SimpleIterator.from_dataframe(\n    df,\n    samplerate=model.samplerate,\n    input_samples=model.input_samples\n)\n</code></pre> <p>Alternative Iterators:</p> <p>While <code>SimpleIterator</code> is convenient, audioclass offers other iterators tailored for specific needs:</p> <ul> <li><code>TFDatasetIterator</code>: Leverages TensorFlow Datasets for enhanced performance on large datasets and parallel processing.</li> </ul> <p>For further details and usage examples, consult the reference documentation.</p>"},{"location":"usage_guide/#whats_next","title":"What's Next?","text":"<p>To unlock the full potential of audioclass, dive into its reference documentation and explore:</p> <ul> <li>Base Interfaces: The   <code>ClipClassificationModel</code>   class and <code>ModelOutput</code> structure provide   the foundation for building custom models and working with predictions.</li> <li>Preprocessing and Postprocessing: Learn how to preprocess your audio data   and convert model outputs into formats suitable for your analysis.</li> <li>Extending audioclass: Discover how to create your own custom models or   batch iterators to fit your specific audio classification needs.</li> </ul>"},{"location":"reference/constants/","title":"Constants","text":""},{"location":"reference/constants/#audioclass.constants","title":"<code>audioclass.constants</code>","text":"<p>Module containing constants for the audioclass package.</p> <p>Attributes:</p> Name Type Description <code>DATA_DIR</code> <code>Path</code> <p>Directory containing supporting data files.</p> <code>DEFAULT_THRESHOLD</code> <code>float</code> <p>Default confidence threshold for model predictions.</p>"},{"location":"reference/constants/#audioclass.constants-attributes","title":"Attributes","text":""},{"location":"reference/constants/#audioclass.constants.BATCH_SIZE","title":"<code>BATCH_SIZE = 4</code>  <code>module-attribute</code>","text":"<p>Default size of batch for model inference.</p> <p>This value determines the number of audio clips that are processed together in a single batch.</p>"},{"location":"reference/constants/#audioclass.constants.DATA_DIR","title":"<code>DATA_DIR = ROOT_DIR / 'data'</code>  <code>module-attribute</code>","text":"<p>Directory containing supporting data files.</p> <p>This is the directory where any additional data files required by the <code>audioclass</code> package are stored.</p>"},{"location":"reference/constants/#audioclass.constants.DEFAULT_THRESHOLD","title":"<code>DEFAULT_THRESHOLD = 0.1</code>  <code>module-attribute</code>","text":"<p>Default confidence threshold for model predictions.</p> <p>This value is used as the minimum probability for a prediction to be considered valid.</p>"},{"location":"reference/constants/#audioclass.constants.ROOT_DIR","title":"<code>ROOT_DIR = Path(__file__).parent</code>  <code>module-attribute</code>","text":"<p>Root directory of the module.</p> <p>This is the directory where the <code>audioclass</code> module is located.</p>"},{"location":"reference/postprocess/","title":"Postprocessing","text":""},{"location":"reference/postprocess/#audioclass.postprocess","title":"<code>audioclass.postprocess</code>","text":"<p>Module for postprocessing audio classification model outputs.</p> <p>This module provides functions to convert raw model outputs (class probabilities and features) into various formats suitable for analysis, visualization, or storage. The primary formats include:</p> <ul> <li>xarray Datasets: Structured datasets containing features, probabilities, and metadata like time, labels, location, and recording time.</li> <li>Lists of soundevent objects: Collections of <code>PredictedTag</code> and <code>Feature</code> objects, compatible with the soundevent library.</li> </ul> <p>These functions facilitate seamless integration with downstream analysis tools and enable flexible representation of audio classification results.</p> <p>Functions:</p> Name Description <code>convert_to_dataset</code> <p>Convert features and class probabilities to an xarray Dataset.</p> <code>convert_to_features_array</code> <p>Convert features to an xarray DataArray.</p> <code>convert_to_features_list</code> <p>Convert a feature array to a list of soundevent <code>Feature</code> objects.</p> <code>convert_to_predicted_tags_list</code> <p>Convert class probabilities to a list of predicted tags.</p> <code>convert_to_probabilities_array</code> <p>Convert class probabilities to a DataArray.</p>"},{"location":"reference/postprocess/#audioclass.postprocess-attributes","title":"Attributes","text":""},{"location":"reference/postprocess/#audioclass.postprocess-functions","title":"Functions","text":""},{"location":"reference/postprocess/#audioclass.postprocess.convert_to_dataset","title":"<code>convert_to_dataset(features, class_probs, labels, hop_size, start_time=0, latitude=None, longitude=None, recorded_on=None, attrs=None)</code>","text":"<p>Convert features and class probabilities to an xarray Dataset.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>ndarray</code> <p>A 2D array of features, where each row corresponds to a frame and each column to a feature.</p> required <code>class_probs</code> <code>ndarray</code> <p>A 2D array of class probabilities, where each row corresponds to a frame and each column to a class.</p> required <code>labels</code> <code>List[str]</code> <p>A list of labels for the classes.</p> required <code>hop_size</code> <code>float</code> <p>The time step between frames in seconds.</p> required <code>start_time</code> <code>float</code> <p>The start time of the first frame in seconds. Defaults to 0.</p> <code>0</code> <code>latitude</code> <code>Optional[float]</code> <p>The latitude of the recording location. Defaults to None.</p> <code>None</code> <code>longitude</code> <code>Optional[float]</code> <p>The longitude of the recording location. Defaults to None.</p> <code>None</code> <code>recorded_on</code> <code>Optional[datetime]</code> <p>The date and time the recording was made. Defaults to None.</p> <code>None</code> <code>attrs</code> <code>Optional[dict]</code> <p>Additional attributes to add to the Dataset. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>Dataset</code> <p>An xarray Dataset containing the features and probabilities as DataArrays, along with coordinates and attributes.</p>"},{"location":"reference/postprocess/#audioclass.postprocess.convert_to_features_array","title":"<code>convert_to_features_array(features, hop_size, start_time=0, latitude=None, longitude=None, recorded_on=None, attrs=None)</code>","text":"<p>Convert features to an xarray DataArray.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>ndarray</code> <p>A 2D array of features, where each row corresponds to a frame and each column to a feature.</p> required <code>hop_size</code> <code>float</code> <p>The time step between frames in seconds.</p> required <code>start_time</code> <code>float</code> <p>The start time of the first frame in seconds. Defaults to 0.</p> <code>0</code> <code>latitude</code> <code>Optional[float]</code> <p>The latitude of the recording location. Defaults to None.</p> <code>None</code> <code>longitude</code> <code>Optional[float]</code> <p>The longitude of the recording location. Defaults to None.</p> <code>None</code> <code>recorded_on</code> <code>Optional[datetime]</code> <p>The date and time the recording was made. Defaults to None.</p> <code>None</code> <code>attrs</code> <code>Optional[dict]</code> <p>Additional attributes to add to the DataArray. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>An xarray DataArray with dimensions <code>time</code> and <code>feature</code>, containing the features.</p>"},{"location":"reference/postprocess/#audioclass.postprocess.convert_to_features_list","title":"<code>convert_to_features_list(features, prefix)</code>","text":"<p>Convert a feature array to a list of soundevent <code>Feature</code> objects.</p> <p>Parameters:</p> Name Type Description Default <code>features</code> <code>ndarray</code> <p>A 2D array of features, where each row corresponds to a frame and each column to a feature.</p> required <code>prefix</code> <code>str</code> <p>A prefix to add to each feature name.</p> required <p>Returns:</p> Type Description <code>List[List[Feature]]</code> <p>A list of lists of <code>Feature</code> objects, where each inner list corresponds to a frame and contains the features for that frame.</p>"},{"location":"reference/postprocess/#audioclass.postprocess.convert_to_predicted_tags_list","title":"<code>convert_to_predicted_tags_list(class_probs, tags, confidence_threshold=DEFAULT_THRESHOLD)</code>","text":"<p>Convert class probabilities to a list of predicted tags.</p> <p>Parameters:</p> Name Type Description Default <code>class_probs</code> <code>ndarray</code> <p>A 2D array of class probabilities, where each row corresponds to a frame and each column to a class.</p> required <code>tags</code> <code>List[Tag]</code> <p>A list of <code>Tag</code> objects representing the possible classes.</p> required <code>confidence_threshold</code> <code>float</code> <p>The minimum probability threshold for a tag to be considered a prediction. Defaults to <code>DEFAULT_THRESHOLD</code>.</p> <code>DEFAULT_THRESHOLD</code> <p>Returns:</p> Type Description <code>List[List[PredictedTag]]</code> <p>A list of lists of <code>PredictedTag</code> objects, where each inner list corresponds to a frame and contains the predicted tags for that frame.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the number of output tags does not match the number of columns in <code>class_probs</code>.</p>"},{"location":"reference/postprocess/#audioclass.postprocess.convert_to_probabilities_array","title":"<code>convert_to_probabilities_array(class_probs, labels, hop_size, start_time=0, latitude=None, longitude=None, recorded_on=None, attrs=None)</code>","text":"<p>Convert class probabilities to a DataArray.</p> <p>Parameters:</p> Name Type Description Default <code>class_probs</code> <code>ndarray</code> <p>A 2D array of class probabilities, where each row corresponds to a frame and each column to a class.</p> required <code>labels</code> <code>List[str]</code> <p>A list of labels for the classes.</p> required <code>hop_size</code> <code>float</code> <p>The time step between frames in seconds.</p> required <code>start_time</code> <code>float</code> <p>The start time of the first frame in seconds. Defaults to 0.</p> <code>0</code> <code>latitude</code> <code>Optional[float]</code> <p>The latitude of the recording location. Defaults to None.</p> <code>None</code> <code>longitude</code> <code>Optional[float]</code> <p>The longitude of the recording location. Defaults to None.</p> <code>None</code> <code>recorded_on</code> <code>Optional[datetime]</code> <p>The date and time the recording was made. Defaults to None.</p> <code>None</code> <code>attrs</code> <code>Optional[dict]</code> <p>Additional attributes to add to the DataArray. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataArray</code> <p>An xarray DataArray with dimensions <code>time</code> and <code>label</code>, containing the class probabilities.</p>"},{"location":"reference/preprocess/","title":"Audio Preprocessing","text":""},{"location":"reference/preprocess/#audioclass.preprocess","title":"<code>audioclass.preprocess</code>","text":"<p>Module for preprocessing audio data.</p> <p>This module helps loading audio data and preprocessing it into a standardized format for audio classification models.</p> <p>Provides functions for loading audio, resampling, and framing into fixed-length buffers.</p> <p>Functions:</p> Name Description <code>load_clip</code> <p>Load an audio clip from a soundevent <code>Clip</code> object.</p> <code>load_recording</code> <p>Load an audio recording from a soundevent <code>Recording</code> object.</p>"},{"location":"reference/preprocess/#audioclass.preprocess-functions","title":"Functions","text":""},{"location":"reference/preprocess/#audioclass.preprocess.load_clip","title":"<code>load_clip(clip, samplerate, buffer_size, audio_dir=None)</code>","text":"<p>Load an audio clip from a soundevent <code>Clip</code> object.</p> <p>This function will load the clip from the audio file, preprocess it, and return a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>clip</code> <code>Clip</code> <p>The soundevent <code>Clip</code> object representing the audio segment.</p> required <code>samplerate</code> <code>int</code> <p>The desired sample rate to resample the audio to.</p> required <code>buffer_size</code> <code>int</code> <p>The length of each audio frame in samples.</p> required <code>audio_dir</code> <code>Optional[Path]</code> <p>The directory containing the audio files. If not provided, the clip's default audio directory is used.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A numpy array of shape (num_frames, buffer_size) containing the preprocessed audio data.</p>"},{"location":"reference/preprocess/#audioclass.preprocess.load_recording","title":"<code>load_recording(recording, samplerate, buffer_size, audio_dir=None)</code>","text":"<p>Load an audio recording from a soundevent <code>Recording</code> object.</p> <p>This function will load the audio file, preprocess it, and return a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>recording</code> <code>Recording</code> <p>The soundevent <code>Recording</code> object representing the audio file.</p> required <code>samplerate</code> <code>int</code> <p>The desired sample rate to resample the audio to.</p> required <code>buffer_size</code> <code>int</code> <p>The length of each audio frame in samples.</p> required <code>audio_dir</code> <code>Optional[Path]</code> <p>The directory containing the audio files. If not provided, the recording's default audio directory is used.</p> <code>None</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>A numpy array of shape (num_frames, buffer_size) containing the preprocessed audio data.</p>"},{"location":"reference/preprocess/#audioclass.preprocess.preprocess_audio","title":"<code>preprocess_audio(wave, samplerate, buffer_size)</code>","text":"<p>Preprocess a loaded audio waveform.</p> <p>This function performs the following preprocessing steps:</p> <ol> <li>Selects the first channel if multiple channels are present.</li> <li>Resamples the audio to the specified sample rate.</li> <li>Frames the audio into fixed-length buffers.</li> </ol> <p>Parameters:</p> Name Type Description Default <code>wave</code> <code>DataArray</code> <p>The loaded audio waveform.</p> required <code>samplerate</code> <code>int</code> <p>The desired sample rate to resample the audio to.</p> required <code>buffer_size</code> <code>int</code> <p>The length of each audio frame in samples.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A numpy array of shape (num_frames, buffer_size) containing the preprocessed audio data.</p>"},{"location":"reference/preprocess/#audioclass.preprocess.resample_audio","title":"<code>resample_audio(wave, samplerate)</code>","text":"<p>Resample audio to a specific sample rate.</p> <p>Parameters:</p> Name Type Description Default <code>wave</code> <code>DataArray</code> <p>The audio waveform to resample.</p> required <code>samplerate</code> <code>int</code> <p>The target sample rate.</p> required <p>Returns:</p> Type Description <code>DataArray</code> <p>The resampled audio waveform.</p>"},{"location":"reference/preprocess/#audioclass.preprocess.stack_array","title":"<code>stack_array(arr, buffer_size)</code>","text":"<p>Stack a 1D array into a 2D array of fixed-length buffers.</p> <p>This function pads the input array with zeros if necessary to ensure that the number of elements is divisible by the buffer size.</p> <p>Parameters:</p> Name Type Description Default <code>arr</code> <code>ndarray</code> <p>The 1D array to stack.</p> required <code>buffer_size</code> <code>int</code> <p>The length of each buffer.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>A 2D array of shape (num_buffers, buffer_size) containing the stacked buffers.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input array has more than one dimension.</p>"},{"location":"reference/utils/","title":"Utilities","text":""},{"location":"reference/utils/#audioclass.utils","title":"<code>audioclass.utils</code>","text":"<p>Utility functions for audioclass.</p> <p>This module provides various helper functions for working with audio data and models.</p> <p>Functions:</p> Name Description <code>batched</code> <p>Batch array data along an axis.</p> <code>flat_sigmoid</code> <p>Apply a flattened sigmoid function to an array.</p> <code>load_artifact</code> <p>Load an artifact from a local path or a URL.</p>"},{"location":"reference/utils/#audioclass.utils-functions","title":"Functions","text":""},{"location":"reference/utils/#audioclass.utils.batched","title":"<code>batched(array, n, axis=0, *, strict=False)</code>","text":"<p>Batch array data along an axis.</p> <p>This function yields batches of data from an array along a specified axis. The final batch may be shorter than the specified batch size if the array length is not divisible by n.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>The input array.</p> required <code>n</code> <code>int</code> <p>The batch size.</p> required <code>strict</code> <code>bool</code> <p>Whether to raise an error if the final batch is shorter than n.</p> <code>False</code> <p>Yields:</p> Name Type Description <code>batch</code> <code>ndarray</code> <p>The next batch of data from the array.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If n is less than 1 or if strict is True and the final batch is shorter than n.</p>"},{"location":"reference/utils/#audioclass.utils.flat_sigmoid","title":"<code>flat_sigmoid(x, sensitivity=1, vmin=-15, vmax=15)</code>","text":"<p>Apply a flattened sigmoid function to an array.</p> <p>This function applies a sigmoid function to each element of the input array, but with a flattened shape to prevent extreme values. The output values are clipped between 0 and 1.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>The input array.</p> required <code>sensitivity</code> <code>float</code> <p>The sensitivity of the sigmoid function. Defaults to 1.</p> <code>1</code> <code>vmin</code> <code>float</code> <p>The minimum value to clip the input to. Defaults to -15.</p> <code>-15</code> <code>vmax</code> <code>float</code> <p>The maximum value to clip the input to. Defaults to 15.</p> <code>15</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>The output array with the same shape as the input.</p>"},{"location":"reference/utils/#audioclass.utils.load_artifact","title":"<code>load_artifact(path, directory=None, download=True)</code>","text":"<p>Load an artifact from a local path or a URL.</p> <p>If the path is a URL, the artifact is downloaded and cached in a local directory. If the artifact is already cached, it is loaded from the cache.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[Path, str]</code> <p>The path to the artifact, either a local path or a URL.</p> required <code>directory</code> <code>Optional[Path]</code> <p>The directory to cache the artifact in. If not provided, a default cache directory is used.</p> <code>None</code> <code>download</code> <code>bool</code> <p>Whether to download the artifact if it is not found in the cache. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>Path</code> <p>The path to the loaded artifact.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the artifact is not found and <code>download</code> is False.</p>"},{"location":"reference/batch/","title":"Batch Processing","text":""},{"location":"reference/batch/#audioclass.batch","title":"<code>audioclass.batch</code>","text":"<p>Module for batch processing audio data for classification tasks.</p> <p>This module provides tools for processing large collections of audio recordings or clips in batches. It offers:</p> <ol> <li> <p>Batch Iterators: Classes like    <code>SimpleIterator</code> and    <code>TFDatasetIterator</code>    generate batches of audio data from various sources (files, directories,                                                         DataFrames).</p> </li> <li> <p>Batch Processing Function: The    <code>process_iterable</code> function applies a    model's processing function to each batch of audio data, streamlining the    classification workflow.</p> </li> </ol>"},{"location":"reference/batch/base/","title":"base","text":""},{"location":"reference/batch/base/#audioclass.batch.base","title":"<code>audioclass.batch.base</code>","text":"<p>Module for defining base classes and functions for batch processing.</p> <p>This module provides abstract classes and utility functions for creating iterators that generate batches of audio data from various sources, such as lists of files, directories, or pandas DataFrames. These iterators are designed to be used with audio classification models to process large amounts of audio data efficiently.</p> <p>Classes:</p> Name Description <code>BaseIterator</code> <p>Abstract base class for audio batch iterators.</p> <p>Functions:</p> Name Description <code>recordings_from_dataframe</code> <p>Create a list of <code>Recording</code> objects from a pandas DataFrame.</p> <code>recordings_from_directory</code> <p>Create a list of <code>Recording</code> objects from audio files in a directory.</p> <code>recordings_from_files</code> <p>Create a list of <code>Recording</code> objects from a list of file paths.</p> <p>Attributes:</p> Name Type Description <code>Batch</code> <code>TypeAlias</code> <p>A single batch of audio data consisting of:</p> <code>BatchGenerator</code> <code>TypeAlias</code> <p>A generator that yields batches of audio data.</p>"},{"location":"reference/batch/base/#audioclass.batch.base-attributes","title":"Attributes","text":""},{"location":"reference/batch/base/#audioclass.batch.base.AudioArray","title":"<code>AudioArray = np.ndarray</code>  <code>module-attribute</code>","text":"<p>A 2D array of audio samples of shape (batch_size, n_samples).</p>"},{"location":"reference/batch/base/#audioclass.batch.base.Batch","title":"<code>Batch = Tuple[AudioArray, List[data.Recording], FrameArray]</code>  <code>module-attribute</code>","text":"<p>A single batch of audio data consisting of:</p> <ul> <li>AudioArray: The audio data as a numpy array.</li> <li>List[data.Recording]: The corresponding list of Recording objects.</li> <li>FrameArray: The frame indices for each audio clip in the batch.</li> </ul>"},{"location":"reference/batch/base/#audioclass.batch.base.BatchGenerator","title":"<code>BatchGenerator = Generator[Batch, None, None]</code>  <code>module-attribute</code>","text":"<p>A generator that yields batches of audio data.</p>"},{"location":"reference/batch/base/#audioclass.batch.base.FrameArray","title":"<code>FrameArray = np.ndarray</code>  <code>module-attribute</code>","text":"<p>A 1D array of frame indices of shape (batch_size, ).</p> <p>These numbers correspond to the frame number in the audio file. In case an audio file is split into multiple frames, this number will be the frame number.</p>"},{"location":"reference/batch/base/#audioclass.batch.base.IndexArray","title":"<code>IndexArray = np.ndarray</code>  <code>module-attribute</code>","text":"<p>A 1D array of indices of shape (batch_size, ).</p> <p>These indices correspond to the index of the file in the list of files being iterated over.</p>"},{"location":"reference/batch/base/#audioclass.batch.base-classes","title":"Classes","text":""},{"location":"reference/batch/base/#audioclass.batch.base.BaseIterator","title":"<code>BaseIterator(recordings, samplerate, input_samples, batch_size=BATCH_SIZE, audio_dir=None)</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for audio batch iterators.</p> <p>This class defines the common interface for iterators that generate batches of audio data from different sources. It provides methods for creating iterators from files, directories, and pandas DataFrames.</p> <p>Parameters:</p> Name Type Description Default <code>recordings</code> <code>List[Recording]</code> <p>A list of <code>Recording</code> objects representing the audio files to be processed.</p> required <code>samplerate</code> <code>int</code> <p>The target sample rate for resampling the audio data (in Hz).</p> required <code>input_samples</code> <code>int</code> <p>The number of samples per audio frame.</p> required <code>batch_size</code> <code>int</code> <p>The number of audio frames per batch. Defaults to <code>BATCH_SIZE</code>.</p> <code>BATCH_SIZE</code> <code>audio_dir</code> <code>Optional[Path]</code> <p>The directory containing the audio files. Defaults to None.</p> <code>None</code> <p>Methods:</p> Name Description <code>from_dataframe</code> <p>Create a batch iterator from a pandas DataFrame.</p> <code>from_directory</code> <p>Create a batch iterator from a directory of audio files.</p> <code>from_files</code> <p>Create a batch iterator from a list of audio files.</p> <p>Attributes:</p> Name Type Description <code>audio_dir</code> <code>Optional[Path]</code> <p>The directory containing the audio files.</p> <code>batch_size</code> <code>int</code> <p>The number of audio frames per batch.</p> <code>input_samples</code> <code>int</code> <p>The number of samples per audio frame.</p> <code>recordings</code> <code>List[Recording]</code> <p>The list of Recording objects to be processed.</p> <code>samplerate</code> <code>int</code> <p>The target sample rate for resampling the audio data (in Hz).</p>"},{"location":"reference/batch/base/#audioclass.batch.base.BaseIterator-attributes","title":"Attributes","text":""},{"location":"reference/batch/base/#audioclass.batch.base.BaseIterator.audio_dir","title":"<code>audio_dir = audio_dir</code>  <code>instance-attribute</code>","text":"<p>The directory containing the audio files.</p>"},{"location":"reference/batch/base/#audioclass.batch.base.BaseIterator.batch_size","title":"<code>batch_size = batch_size</code>  <code>instance-attribute</code>","text":"<p>The number of audio frames per batch.</p>"},{"location":"reference/batch/base/#audioclass.batch.base.BaseIterator.input_samples","title":"<code>input_samples = input_samples</code>  <code>instance-attribute</code>","text":"<p>The number of samples per audio frame.</p>"},{"location":"reference/batch/base/#audioclass.batch.base.BaseIterator.recordings","title":"<code>recordings = recordings</code>  <code>instance-attribute</code>","text":"<p>The list of Recording objects to be processed.</p>"},{"location":"reference/batch/base/#audioclass.batch.base.BaseIterator.samplerate","title":"<code>samplerate = samplerate</code>  <code>instance-attribute</code>","text":"<p>The target sample rate for resampling the audio data (in Hz).</p>"},{"location":"reference/batch/base/#audioclass.batch.base.BaseIterator-functions","title":"Functions","text":""},{"location":"reference/batch/base/#audioclass.batch.base.BaseIterator.from_dataframe","title":"<code>from_dataframe(df, samplerate, input_samples, batch_size=BATCH_SIZE, audio_dir=None, path_col='path', latitude_col='latitude', longitude_col='longitude', recorded_on_col='recorded_on', additional_cols=None)</code>  <code>classmethod</code>","text":"<p>Create a batch iterator from a pandas DataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A DataFrame containing information about the audio files.</p> required <code>samplerate</code> <code>int</code> <p>The target sample rate for resampling the audio data (in Hz).</p> required <code>input_samples</code> <code>int</code> <p>The number of samples per audio frame.</p> required <code>batch_size</code> <code>int</code> <p>The number of audio frames per batch. Defaults to <code>BATCH_SIZE</code>.</p> <code>BATCH_SIZE</code> <code>audio_dir</code> <code>Optional[Path]</code> <p>The directory containing the audio files. Defaults to None.</p> <code>None</code> <code>path_col</code> <code>str</code> <p>The name of the column in the DataFrame containing the paths to the audio files. Defaults to \"path\".</p> <code>'path'</code> <code>latitude_col</code> <code>Optional[str]</code> <p>The name of the column in the DataFrame containing the latitudes of the recording locations. Defaults to \"latitude\".</p> <code>'latitude'</code> <code>longitude_col</code> <code>Optional[str]</code> <p>The name of the column in the DataFrame containing the longitudes of the recording locations. Defaults to \"longitude\".</p> <code>'longitude'</code> <code>recorded_on_col</code> <code>Optional[str]</code> <p>The name of the column in the DataFrame containing the recording timestamps. Defaults to \"recorded_on\".</p> <code>'recorded_on'</code> <code>additional_cols</code> <code>Optional[list[str]]</code> <p>A list of additional columns in the DataFrame to include as tags in the <code>Recording</code> objects. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>BaseIterator</code> <p>A batch iterator for the audio files specified in the DataFrame.</p>"},{"location":"reference/batch/base/#audioclass.batch.base.BaseIterator.from_directory","title":"<code>from_directory(directory, samplerate, input_samples, batch_size=BATCH_SIZE, recursive=True)</code>  <code>classmethod</code>","text":"<p>Create a batch iterator from a directory of audio files.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Path</code> <p>The path to the directory containing the audio files.</p> required <code>samplerate</code> <code>int</code> <p>The target sample rate for resampling the audio data (in Hz).</p> required <code>input_samples</code> <code>int</code> <p>The number of samples per audio frame.</p> required <code>batch_size</code> <code>int</code> <p>The number of audio frames per batch. Defaults to <code>BATCH_SIZE</code>.</p> <code>BATCH_SIZE</code> <code>recursive</code> <code>bool</code> <p>Whether to search for audio files recursively in subdirectories. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>BaseIterator</code> <p>A batch iterator for the audio files in the specified directory.</p>"},{"location":"reference/batch/base/#audioclass.batch.base.BaseIterator.from_files","title":"<code>from_files(files, samplerate, input_samples, batch_size=BATCH_SIZE, audio_dir=None)</code>  <code>classmethod</code>","text":"<p>Create a batch iterator from a list of audio files.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>List[Path]</code> <p>A list of paths to the audio files.</p> required <code>samplerate</code> <code>int</code> <p>The target sample rate for resampling the audio data (in Hz).</p> required <code>input_samples</code> <code>int</code> <p>The number of samples per audio frame.</p> required <code>batch_size</code> <code>int</code> <p>The number of audio frames per batch. Defaults to <code>BATCH_SIZE</code>.</p> <code>BATCH_SIZE</code> <code>audio_dir</code> <code>Optional[Path]</code> <p>The directory containing the audio files. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>BaseIterator</code> <p>A batch iterator for the specified audio files.</p>"},{"location":"reference/batch/base/#audioclass.batch.base-functions","title":"Functions","text":""},{"location":"reference/batch/base/#audioclass.batch.base.recordings_from_dataframe","title":"<code>recordings_from_dataframe(df, path_col='path', latitude_col='latitude', longitude_col='longitude', recorded_on_col='recorded_on', additional_cols=None)</code>","text":"<p>Create a list of <code>Recording</code> objects from a pandas DataFrame.</p> <p>The DataFrame should contain a column with file paths (specified by <code>path_col</code>), and optionally columns for latitude, longitude, recorded_on timestamp, and any additional columns to be included as tags in the <code>Recording</code> objects.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>A pandas DataFrame containing information about the audio files.</p> required <code>path_col</code> <code>str</code> <p>The name of the column containing the file paths. Defaults to \"path\".</p> <code>'path'</code> <code>latitude_col</code> <code>Optional[str]</code> <p>The name of the column containing the latitudes. Defaults to \"latitude\".</p> <code>'latitude'</code> <code>longitude_col</code> <code>Optional[str]</code> <p>The name of the column containing the longitudes. Defaults to \"longitude\".</p> <code>'longitude'</code> <code>recorded_on_col</code> <code>Optional[str]</code> <p>The name of the column containing the recorded_on timestamps. Defaults to \"recorded_on\".</p> <code>'recorded_on'</code> <code>additional_cols</code> <code>Optional[list[str]]</code> <p>A list of additional column names to include as tags in the <code>Recording</code> objects. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[Recording]</code> <p>A list of <code>Recording</code> objects corresponding to the rows in the DataFrame.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the DataFrame does not contain the required columns.</p>"},{"location":"reference/batch/base/#audioclass.batch.base.recordings_from_directory","title":"<code>recordings_from_directory(directory, recursive=True)</code>","text":"<p>Create a list of <code>Recording</code> objects from audio files in a directory.</p> <p>Parameters:</p> Name Type Description Default <code>directory</code> <code>Path</code> <p>The path to the directory containing the audio files.</p> required <code>recursive</code> <code>bool</code> <p>Whether to search for audio files recursively in subdirectories. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>List[Recording]</code> <p>A list of <code>Recording</code> objects corresponding to the audio files found in the directory.</p>"},{"location":"reference/batch/base/#audioclass.batch.base.recordings_from_files","title":"<code>recordings_from_files(files, ignore_errors=True)</code>","text":"<p>Create a list of <code>Recording</code> objects from a list of file paths.</p> <p>This function iterates through a list of file paths, creating a <code>soundevent.data.Recording</code> object for each file. It can optionally ignore errors that occur during file processing.</p> <p>Parameters:</p> Name Type Description Default <code>files</code> <code>List[Path]</code> <p>A list of <code>pathlib.Path</code> objects pointing to audio files.</p> required <code>ignore_errors</code> <code>bool</code> <p>If True, any errors encountered while creating a <code>Recording</code> from a file will be ignored, and the file will be skipped. If False, any error will be raised. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>List[Recording]</code> <p>A list of <code>Recording</code> objects created from the provided files.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If <code>ignore_errors</code> is False and an error occurs while processing a file.</p>"},{"location":"reference/batch/process/","title":"process","text":""},{"location":"reference/batch/process/#audioclass.batch.process","title":"<code>audioclass.batch.process</code>","text":"<p>Module for batch processing audio data with classification models.</p> <p>Functions:</p> Name Description <code>process_iterable</code> <p>Process an iterable of audio data using a models <code>process_array</code> method.</p>"},{"location":"reference/batch/process/#audioclass.batch.process-attributes","title":"Attributes","text":""},{"location":"reference/batch/process/#audioclass.batch.process-classes","title":"Classes","text":""},{"location":"reference/batch/process/#audioclass.batch.process-functions","title":"Functions","text":""},{"location":"reference/batch/process/#audioclass.batch.process.process_iterable","title":"<code>process_iterable(process_array, iterable, tags, name, confidence_threshold=DEFAULT_THRESHOLD)</code>","text":"<p>Process an iterable of audio data using a models <code>process_array</code> method.</p> <p>This function iterates over batches of audio clips, processes each clip using the provided <code>process_array</code> function, and returns a list of <code>ClipPrediction</code> objects. The <code>process_array</code> function should take a numpy array of audio data and return a tuple of class probabilities and extracted features.</p> <p>Parameters:</p> Name Type Description Default <code>process_array</code> <code>Callable[[ndarray], Tuple[ndarray, ndarray]]</code> <p>A function that takes a numpy array of audio data and returns a tuple of class probabilities and extracted features.</p> required <code>iterable</code> <code>BaseIterator</code> <p>An iterator that yields batches of audio clips.</p> required <code>tags</code> <code>List[Tag]</code> <p>A list of tags that the model can predict.</p> required <code>name</code> <code>str</code> <p>The name of the model.</p> required <code>confidence_threshold</code> <code>float</code> <p>The minimum confidence threshold for a tag to be included in a prediction. Defaults to <code>DEFAULT_THRESHOLD</code>.</p> <code>DEFAULT_THRESHOLD</code> <p>Returns:</p> Type Description <code>List[ClipPrediction]</code> <p>A list of <code>ClipPrediction</code> objects, one for each audio clip.</p>"},{"location":"reference/batch/simple/","title":"simple","text":""},{"location":"reference/batch/simple/#audioclass.batch.simple","title":"<code>audioclass.batch.simple</code>","text":"<p>Module for providing a simple audio batch iterator.</p> <p>Classes:</p> Name Description <code>SimpleIterator</code> <p>A straightforward iterator for processing audio recordings in batches.</p>"},{"location":"reference/batch/simple/#audioclass.batch.simple-attributes","title":"Attributes","text":""},{"location":"reference/batch/simple/#audioclass.batch.simple-classes","title":"Classes","text":""},{"location":"reference/batch/simple/#audioclass.batch.simple.SimpleIterator","title":"<code>SimpleIterator(recordings, samplerate, input_samples, batch_size=BATCH_SIZE, audio_dir=None)</code>","text":"<p>               Bases: <code>BaseIterator</code></p> <p>A straightforward iterator for processing audio recordings in batches.</p> <p>This iterator loads and preprocesses audio recordings one at a time, then groups them into batches for efficient processing by machine learning models.</p> <p>It's ideal for smaller datasets.</p> Notes <p>For larger datasets or situations requiring parallel processing, consider using alternative iterators like <code>TFDatasetIterator</code> or a custom implementation.</p>"},{"location":"reference/batch/simple/#audioclass.batch.simple-functions","title":"Functions","text":""},{"location":"reference/batch/tensorflow/","title":"tensorflow","text":"<p>Warning</p> <p>Additional Dependencies: To use the Tensorflow iterator, you will need to install additional dependencies. These dependencies are optional, as they can be heavy and may not be needed in all use cases. To install them, run:</p> <pre><code>pip install \"audioclass[tensorflow]\"\n</code></pre>"},{"location":"reference/batch/tensorflow/#audioclass.batch.tensorflow","title":"<code>audioclass.batch.tensorflow</code>","text":"<p>Module for providing a TensorFlow Dataset-based audio batch iterator.</p> <p>Classes:</p> Name Description <code>TFDatasetIterator</code> <p>A TensorFlow Dataset-based audio batch iterator.</p>"},{"location":"reference/batch/tensorflow/#audioclass.batch.tensorflow-attributes","title":"Attributes","text":""},{"location":"reference/batch/tensorflow/#audioclass.batch.tensorflow-classes","title":"Classes","text":""},{"location":"reference/batch/tensorflow/#audioclass.batch.tensorflow.TFDatasetIterator","title":"<code>TFDatasetIterator(recordings, samplerate, input_samples, batch_size=BATCH_SIZE, audio_dir=None)</code>","text":"<p>               Bases: <code>BaseIterator</code></p> <p>A TensorFlow Dataset-based audio batch iterator.</p> <p>This iterator leverages TensorFlow's Dataset API to efficiently load, preprocess, and batch audio data for model training and inference. It provides parallel loading and preprocessing capabilities, making it suitable for large datasets.</p>"},{"location":"reference/batch/tensorflow/#audioclass.batch.tensorflow-functions","title":"Functions","text":""},{"location":"reference/models/","title":"Models","text":""},{"location":"reference/models/#audioclass.models","title":"<code>audioclass.models</code>","text":"<p>Module for defining and providing audio classification models.</p> <p>This module establishes the core interface for audio classification models within the audioclass library. It defines the <code>ClipClassificationModel</code> abstract base class, which serves as a blueprint for creating different model implementations (e.g., TensorFlow, PyTorch, etc.). This ensures consistency in how models are used and integrated into audio classification workflows.</p> <p>The module also provides concrete implementations of audio classification models, allowing users to directly apply them to their audio data. These models adhere to the standardized interface defined by the <code>ClipClassificationModel</code> class, making them easy to use and interchangeable within the library.</p> <p>Additionally, this module defines the <code>ModelOutput</code> namedtuple, specifying the standard format for the output produced by audio classification models. This ensures that all models within the library produce results in a consistent and predictable way, facilitating further analysis and post-processing.</p>"},{"location":"reference/models/#model_types","title":"Model Types","text":"<ul> <li>Tensorflow Models</li> <li>TFLite Models</li> </ul>"},{"location":"reference/models/#models_1","title":"Models","text":"<ul> <li>Google Perch</li> <li>BirdNET</li> </ul>"},{"location":"reference/models/base/","title":"base","text":""},{"location":"reference/models/base/#audioclass.models.base","title":"<code>audioclass.models.base</code>","text":"<p>Module defining the base classes for audio classification models and their output format.</p> <p>This module provides abstract classes for clip classification models, establishing a standard interface for model input, processing, and output. It also defines the structure of the model output, which includes class probabilities and extracted features.</p> <p>Classes:</p> Name Description <code>ClipClassificationModel</code> <p>Abstract base class for audio clip classification models.</p> <code>ModelOutput</code> <p>Output format for audio classification models.</p>"},{"location":"reference/models/base/#audioclass.models.base-classes","title":"Classes","text":""},{"location":"reference/models/base/#audioclass.models.base.ClipClassificationModel","title":"<code>ClipClassificationModel</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for audio clip classification models.</p> <p>This class defines the common interface for audio classification models that process individual clips. It provides methods for processing raw audio arrays, files, recordings, and clips, as well as an iterable of clips.</p> <p>Methods:</p> Name Description <code>process_array</code> <p>Process a single audio array and return the model output.</p> <code>process_clip</code> <p>Process an audio clip and return the model output.</p> <code>process_file</code> <p>Process an audio file and return the model output.</p> <code>process_iterable</code> <p>Process an iterable of audio clips and return a list of predictions.</p> <code>process_recording</code> <p>Process an audio recording and return the model output.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>int</code> <p>The maximum number of framces to process in each batch.</p> <code>confidence_threshold</code> <code>float</code> <p>The minimum confidence threshold for a class to be considered.</p> <code>input_samples</code> <code>int</code> <p>The number of audio samples expected in each input frame.</p> <code>name</code> <code>str</code> <p>The name of the model.</p> <code>num_classes</code> <code>int</code> <p>The number of classes that the model can predict.</p> <code>samplerate</code> <code>int</code> <p>The sample rate of the audio data expected by the model (in Hz).</p> <code>tags</code> <code>List[Tag]</code> <p>The list of tags that the model can predict.</p>"},{"location":"reference/models/base/#audioclass.models.base.ClipClassificationModel-attributes","title":"Attributes","text":""},{"location":"reference/models/base/#audioclass.models.base.ClipClassificationModel.batch_size","title":"<code>batch_size = 8</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The maximum number of framces to process in each batch.</p>"},{"location":"reference/models/base/#audioclass.models.base.ClipClassificationModel.confidence_threshold","title":"<code>confidence_threshold</code>  <code>instance-attribute</code>","text":"<p>The minimum confidence threshold for a class to be considered.</p>"},{"location":"reference/models/base/#audioclass.models.base.ClipClassificationModel.input_samples","title":"<code>input_samples</code>  <code>instance-attribute</code>","text":"<p>The number of audio samples expected in each input frame.</p>"},{"location":"reference/models/base/#audioclass.models.base.ClipClassificationModel.name","title":"<code>name</code>  <code>instance-attribute</code>","text":"<p>The name of the model.</p>"},{"location":"reference/models/base/#audioclass.models.base.ClipClassificationModel.num_classes","title":"<code>num_classes</code>  <code>instance-attribute</code>","text":"<p>The number of classes that the model can predict.</p>"},{"location":"reference/models/base/#audioclass.models.base.ClipClassificationModel.samplerate","title":"<code>samplerate</code>  <code>instance-attribute</code>","text":"<p>The sample rate of the audio data expected by the model (in Hz).</p>"},{"location":"reference/models/base/#audioclass.models.base.ClipClassificationModel.tags","title":"<code>tags</code>  <code>instance-attribute</code>","text":"<p>The list of tags that the model can predict.</p>"},{"location":"reference/models/base/#audioclass.models.base.ClipClassificationModel-functions","title":"Functions","text":""},{"location":"reference/models/base/#audioclass.models.base.ClipClassificationModel.process_array","title":"<code>process_array(array)</code>  <code>abstractmethod</code>","text":"<p>Process a single audio array and return the model output.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>The audio array to be processed, with shape <code>(num_frames, input_samples)</code>.</p> required <p>Returns:</p> Type Description <code>ModelOutput</code> <p>A <code>ModelOutput</code> object containing the class probabilities and extracted features.</p>"},{"location":"reference/models/base/#audioclass.models.base.ClipClassificationModel.process_clip","title":"<code>process_clip(clip, fmt='soundevent')</code>","text":"<pre><code>process_clip(clip: data.Clip, fmt: Literal['soundevent'] = 'soundevent') -&gt; List[data.ClipPrediction]\n</code></pre><pre><code>process_clip(clip: data.Clip, fmt: Literal['dataset'] = 'dataset') -&gt; xr.Dataset\n</code></pre> <p>Process an audio clip and return the model output.</p> <p>Parameters:</p> Name Type Description Default <code>clip</code> <code>Clip</code> <p>The <code>Clip</code> object representing the audio segment.</p> required <code>fmt</code> <code>Literal['soundevent', 'dataset']</code> <p>The desired output format. \"soundevent\" returns a list of <code>ClipPrediction</code> objects, while \"dataset\" returns an xarray <code>Dataset</code>. Defaults to \"soundevent\".</p> <code>'soundevent'</code> <p>Returns:</p> Type Description <code>Union[List[ClipPrediction], Dataset]</code> <p>The model output in the specified format.</p>"},{"location":"reference/models/base/#audioclass.models.base.ClipClassificationModel.process_file","title":"<code>process_file(path, fmt='soundevent', **kwargs)</code>","text":"<pre><code>process_file(path: Path, fmt: Literal['soundevent'] = 'soundevent', **kwargs) -&gt; List[data.ClipPrediction]\n</code></pre><pre><code>process_file(path: Path, fmt: Literal['dataset'] = 'dataset', **kwargs) -&gt; xr.Dataset\n</code></pre> <p>Process an audio file and return the model output.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Path</code> <p>The path to the audio file.</p> required <code>fmt</code> <code>Literal['soundevent', 'dataset']</code> <p>The desired output format. \"soundevent\" returns a list of <code>ClipPrediction</code> objects, while \"dataset\" returns an xarray <code>Dataset</code>. Defaults to \"soundevent\".</p> <code>'soundevent'</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to <code>Recording.from_file()</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[List[ClipPrediction], Dataset]</code> <p>The model output in the specified format.</p>"},{"location":"reference/models/base/#audioclass.models.base.ClipClassificationModel.process_iterable","title":"<code>process_iterable(iterable)</code>","text":"<p>Process an iterable of audio clips and return a list of predictions.</p> <p>Parameters:</p> Name Type Description Default <code>iterable</code> <code>BaseIterator</code> <p>An iterator that yields <code>Clip</code> objects.</p> required <p>Returns:</p> Type Description <code>List[ClipPrediction]</code> <p>A list of <code>ClipPrediction</code> objects, one for each clip in the iterable.</p>"},{"location":"reference/models/base/#audioclass.models.base.ClipClassificationModel.process_recording","title":"<code>process_recording(recording, fmt='soundevent')</code>","text":"<pre><code>process_recording(recording: data.Recording, fmt: Literal['soundevent'] = 'soundevent') -&gt; List[data.ClipPrediction]\n</code></pre><pre><code>process_recording(recording: data.Recording, fmt: Literal['dataset'] = 'dataset') -&gt; xr.Dataset\n</code></pre> <p>Process an audio recording and return the model output.</p> <p>Parameters:</p> Name Type Description Default <code>recording</code> <code>Recording</code> <p>The <code>Recording</code> object representing the audio.</p> required <code>fmt</code> <code>Literal['soundevent', 'dataset']</code> <p>The desired output format. \"soundevent\" returns a list of <code>ClipPrediction</code> objects, while \"dataset\" returns an xarray <code>Dataset</code>. Defaults to \"soundevent\".</p> <code>'soundevent'</code> <p>Returns:</p> Type Description <code>Union[List[ClipPrediction], Dataset]</code> <p>The model output in the specified format.</p>"},{"location":"reference/models/base/#audioclass.models.base.ModelOutput","title":"<code>ModelOutput</code>","text":"<p>               Bases: <code>NamedTuple</code></p> <p>Output format for audio classification models.</p> <p>Attributes:</p> Name Type Description <code>class_probs</code> <code>ndarray</code> <p>Array of class probabilities for each frame.</p> <code>features</code> <code>ndarray</code> <p>Array of extracted features for each frame.</p>"},{"location":"reference/models/base/#audioclass.models.base.ModelOutput-attributes","title":"Attributes","text":""},{"location":"reference/models/base/#audioclass.models.base.ModelOutput.class_probs","title":"<code>class_probs</code>  <code>instance-attribute</code>","text":"<p>Array of class probabilities for each frame.</p> <p>The array has shape <code>(num_frames, num_classes)</code>, where <code>num_frames</code> is the number of frames in the input audio clip and <code>num_classes</code> is the number of classes that the model can predict.</p> <p>Notice that the interpretation may vary depending on the model and it is advisable to check the model's documentation for more information.</p>"},{"location":"reference/models/base/#audioclass.models.base.ModelOutput.features","title":"<code>features</code>  <code>instance-attribute</code>","text":"<p>Array of extracted features for each frame.</p> <p>The array has shape <code>(num_frames, num_features)</code>, where <code>num_frames</code> is the number of frames in the input audio clip and <code>num_features</code> is the number of features extracted by the model.</p> <p>The features can be used for further analysis or visualization of the model output.</p>"},{"location":"reference/models/base/#audioclass.models.base-functions","title":"Functions","text":""},{"location":"reference/models/birdnet/","title":"birdnet","text":"<p>Warning</p> <p>Additional Dependencies: To use the BirdNET model, you will need to install additional dependencies. These dependencies are optional, as they can be heavy and may not be needed in all use cases. To install them, run:</p> <pre><code>pip install \"audioclass[birdnet]\"\n</code></pre>"},{"location":"reference/models/birdnet/#audioclass.models.birdnet","title":"<code>audioclass.models.birdnet</code>","text":"<p>Module for loading and using the BirdNET audio classification model.</p> <p>This module provides a convenient interface for working with the BirdNET model, which is a TensorFlow Lite-based model designed for bird sound classification. It includes the <code>BirdNET</code> class, which is a subclass of <code>TFLiteModel</code>, and functions for loading the model and its associated labels.</p> Notes <p>The BirdNET model was developed by the K. Lisa Yang Center for Conservation Bioacoustics at the Cornell Lab of Ornithology, in collaboration with Chemnitz University of Technology. This package is not affiliated with the BirdNET project.</p> <p>BirdNET is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License.</p> <p>If you use the BirdNET model, please cite:</p> <pre><code>Kahl, S., Wood, C. M., Eibl, M., &amp; Klinck, H. (2021). BirdNET: A deep\nlearning solution for avian diversity monitoring. Ecological Informatics,\n61, 101236.\n</code></pre> <p>For further details, please visit the official BirdNET repository</p> <p>Classes:</p> Name Description <code>BirdNET</code> <p>BirdNET audio classification model.</p>"},{"location":"reference/models/birdnet/#audioclass.models.birdnet-attributes","title":"Attributes","text":""},{"location":"reference/models/birdnet/#audioclass.models.birdnet.INPUT_SAMPLES","title":"<code>INPUT_SAMPLES = 144000</code>  <code>module-attribute</code>","text":"<p>Default number of samples expected in the input tensor.</p> <p>This value corresponds to 3 seconds of audio data at a sample rate of 48,000 Hz.</p>"},{"location":"reference/models/birdnet/#audioclass.models.birdnet.LABELS_PATH","title":"<code>LABELS_PATH = 'https://github.com/birdnet-team/BirdNET-Analyzer/raw/refs/tags/v2.0.0/birdnet_analyzer/labels/V2.4/BirdNET_GLOBAL_6K_V2.4_Labels_en_uk.txt'</code>  <code>module-attribute</code>","text":"<p>Default path to the BirdNET labels file.</p>"},{"location":"reference/models/birdnet/#audioclass.models.birdnet.MODEL_PATH","title":"<code>MODEL_PATH = 'https://github.com/birdnet-team/BirdNET-Analyzer/raw/refs/tags/v1.5.1/birdnet_analyzer/checkpoints/V2.4/BirdNET_GLOBAL_6K_V2.4_Model_FP32.tflite'</code>  <code>module-attribute</code>","text":"<p>Default path to the BirdNET TensorFlow Lite model file.</p>"},{"location":"reference/models/birdnet/#audioclass.models.birdnet.SAMPLERATE","title":"<code>SAMPLERATE = 48000</code>  <code>module-attribute</code>","text":"<p>Default sample rate of the audio data expected by the model (in Hz).</p> <p>This value corresponds to the sample rate used by the BirdNET model.</p>"},{"location":"reference/models/birdnet/#audioclass.models.birdnet-classes","title":"Classes","text":""},{"location":"reference/models/birdnet/#audioclass.models.birdnet.BirdNET","title":"<code>BirdNET(interpreter, signature, tags, confidence_threshold, samplerate, name, logits=True, batch_size=8)</code>","text":"<p>               Bases: <code>TFLiteModel</code></p> <p>BirdNET audio classification model.</p> <p>This class is a wrapper around a TensorFlow Lite model for bird sound classification. It provides methods for loading the model, processing audio data, and returning predictions.</p> <p>Methods:</p> Name Description <code>load</code> <p>Load a BirdNET model from a file or URL.</p>"},{"location":"reference/models/birdnet/#audioclass.models.birdnet.BirdNET-functions","title":"Functions","text":""},{"location":"reference/models/birdnet/#audioclass.models.birdnet.BirdNET.load","title":"<code>load(model_path=MODEL_PATH, labels_path=LABELS_PATH, num_threads=None, confidence_threshold=DEFAULT_THRESHOLD, samplerate=SAMPLERATE, name='BirdNET', common_name=False, batch_size=8)</code>  <code>classmethod</code>","text":"<p>Load a BirdNET model from a file or URL.</p> <p>Parameters:</p> Name Type Description Default <code>model_path</code> <code>Union[Path, str]</code> <p>The path or URL to the TensorFlow Lite model file. Defaults to the latest version of the BirdNET model.</p> <code>MODEL_PATH</code> <code>labels_path</code> <code>Union[Path, str]</code> <p>The path or URL to the labels file. Defaults to the latest version of the BirdNET labels.</p> <code>LABELS_PATH</code> <code>num_threads</code> <code>Optional[int]</code> <p>The number of threads to use for inference. If None, the default number of threads will be used.</p> <code>None</code> <code>confidence_threshold</code> <code>float</code> <p>The minimum confidence threshold for making predictions. Defaults to <code>DEFAULT_THRESHOLD</code>.</p> <code>DEFAULT_THRESHOLD</code> <code>samplerate</code> <code>int</code> <p>The sample rate of the audio data expected by the model (in Hz). Defaults to 48,000 Hz.</p> <code>SAMPLERATE</code> <code>name</code> <code>str</code> <p>The name of the model. Defaults to \"BirdNET\".</p> <code>'BirdNET'</code> <code>common_name</code> <code>bool</code> <p>Whether to use common names for bird species instead of scientific names. Defaults to False.</p> <code>False</code> <code>batch_size</code> <code>int</code> <p>The number of samples to process in each batch. Defaults to 8.</p> <code>8</code> <p>Returns:</p> Type Description <code>BirdNET</code> <p>An instance of the BirdNET class.</p>"},{"location":"reference/models/birdnet/#audioclass.models.birdnet-functions","title":"Functions","text":""},{"location":"reference/models/birdnet/#audioclass.models.birdnet.get_signature","title":"<code>get_signature(interpreter)</code>","text":"<p>Get the signature of a BirdNET model.</p> <p>Parameters:</p> Name Type Description Default <code>interpreter</code> <code>Interpreter</code> <p>The TensorFlow Lite interpreter object.</p> required <p>Returns:</p> Type Description <code>Signature</code> <p>The signature of the BirdNET model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model signature does not match the expected format.</p>"},{"location":"reference/models/birdnet/#audioclass.models.birdnet.load_tags","title":"<code>load_tags(path=LABELS_PATH, common_name=False)</code>","text":"<p>Load BirdNET labels from a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[Path, str]</code> <p>Path or URL to the file containing the labels. Defaults to the latest version of the BirdNET labels.</p> <code>LABELS_PATH</code> <code>common_name</code> <code>bool</code> <p>Whether to return the common name instead of the scientific name. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[Tag]</code> <p>List of soundevent <code>Tag</code> objects.</p>"},{"location":"reference/models/birdnet_analyzer/","title":"birdnet analyzer","text":"<p>Warning</p> <p>Additional Dependencies: To use the BirdNET model, you will need to install additional dependencies. These dependencies are optional, as they can be heavy and may not be needed in all use cases. To install them, run:</p> <pre><code>pip install \"audioclass[birdnet_analyzer]\"\n</code></pre>"},{"location":"reference/models/birdnet_analyzer/#audioclass.models.birdnet_analyzer","title":"<code>audioclass.models.birdnet_analyzer</code>","text":"<p>Audio Classification Module for Bird Sound Detection.</p> <p>This module provides tools for loading and using the BirdNET sound event detection model. BirdNET is a deep learning model designed to classify bird species from audio recordings.</p> <p>Classes:</p> Name Description <code>BirdNETAnalyzer</code> <p>BirdNET sound event detection model.</p> <p>Functions:</p> Name Description <code>load_tags</code> <p>Load BirdNET labels from a file.</p> <p>Attributes:</p> Name Type Description <code>INPUT_SAMPLES</code> <p>Default number of samples expected in the input tensor.</p> <code>SAMPLERATE</code> <p>Default sample rate of the audio data expected by the model (in Hz).</p> <code>SAVED_MODEL_PATH</code>"},{"location":"reference/models/birdnet_analyzer/#audioclass.models.birdnet_analyzer-attributes","title":"Attributes","text":""},{"location":"reference/models/birdnet_analyzer/#audioclass.models.birdnet_analyzer.INPUT_SAMPLES","title":"<code>INPUT_SAMPLES = 144000</code>  <code>module-attribute</code>","text":"<p>Default number of samples expected in the input tensor.</p> <p>This value corresponds to 3 seconds of audio data at a sample rate of 48,000 Hz.</p>"},{"location":"reference/models/birdnet_analyzer/#audioclass.models.birdnet_analyzer.SAMPLERATE","title":"<code>SAMPLERATE = 48000</code>  <code>module-attribute</code>","text":"<p>Default sample rate of the audio data expected by the model (in Hz).</p> <p>This value corresponds to the sample rate used by the BirdNET model.</p>"},{"location":"reference/models/birdnet_analyzer/#audioclass.models.birdnet_analyzer.SAVED_MODEL_PATH","title":"<code>SAVED_MODEL_PATH = DATA_DIR / 'BirdNET_GLOBAL_6K_V2.4'</code>  <code>module-attribute</code>","text":""},{"location":"reference/models/birdnet_analyzer/#audioclass.models.birdnet_analyzer-classes","title":"Classes","text":""},{"location":"reference/models/birdnet_analyzer/#audioclass.models.birdnet_analyzer.BirdNETAnalyzer","title":"<code>BirdNETAnalyzer(callable, signature, tags, confidence_threshold, samplerate, name, logits=True, batch_size=8)</code>","text":"<p>               Bases: <code>TensorflowModel</code></p> <p>BirdNET sound event detection model.</p> <p>This class loads and wraps the BirdNET TensorFlow SavedModel for bird sound classification and embedding extraction.</p> <p>Methods:</p> Name Description <code>load</code> <p>Load a BirdNET model from a saved model directory.</p>"},{"location":"reference/models/birdnet_analyzer/#audioclass.models.birdnet_analyzer.BirdNETAnalyzer-functions","title":"Functions","text":""},{"location":"reference/models/birdnet_analyzer/#audioclass.models.birdnet_analyzer.BirdNETAnalyzer.load","title":"<code>load(model_url=SAVED_MODEL_PATH, tags_url=LABELS_PATH, confidence_threshold=DEFAULT_THRESHOLD, samplerate=SAMPLERATE, name='BirdNET', common_name=False, batch_size=8)</code>  <code>classmethod</code>","text":"<p>Load a BirdNET model from a saved model directory.</p> <p>Parameters:</p> Name Type Description Default <code>model_url</code> <code>Union[Path, str]</code> <p>The path to the saved model directory. Defaults to a local copy of the BirdNET model.</p> <code>SAVED_MODEL_PATH</code> <code>tags_url</code> <code>Union[Path, str]</code> <p>The URL or path to the file containing the labels. Defaults to the labels file in the BirdNET repository.</p> <code>LABELS_PATH</code> <code>confidence_threshold</code> <code>float</code> <p>The minimum confidence threshold for making predictions. Defaults to <code>DEFAULT_THRESHOLD</code>.</p> <code>DEFAULT_THRESHOLD</code> <code>samplerate</code> <code>int</code> <p>The sample rate of the audio data expected by the model (in Hz). Defaults to <code>SAMPLERATE</code>.</p> <code>SAMPLERATE</code> <code>name</code> <code>str</code> <p>The name of the model. Defaults to \"BirdNET\".</p> <code>'BirdNET'</code> <p>Returns:</p> Type Description <code>BirdNETAnalyzer</code> <p>An instance of the BirdNET model.</p>"},{"location":"reference/models/birdnet_analyzer/#audioclass.models.birdnet_analyzer-functions","title":"Functions","text":""},{"location":"reference/models/birdnet_analyzer/#audioclass.models.birdnet_analyzer.load_tags","title":"<code>load_tags(path=LABELS_PATH, common_name=False)</code>","text":"<p>Load BirdNET labels from a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[Path, str]</code> <p>Path or URL to the file containing the labels. Defaults to the latest version of the BirdNET labels.</p> <code>LABELS_PATH</code> <code>common_name</code> <code>bool</code> <p>Whether to return the common name instead of the scientific name. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[Tag]</code> <p>List of soundevent <code>Tag</code> objects.</p>"},{"location":"reference/models/perch/","title":"perch","text":"<p>Warning</p> <p>Additional Dependencies: To use the Perch model, you will need to install additional dependencies. These dependencies are optional, as they can be heavy and may not be needed in all use cases. To install them, run:</p> <pre><code>pip install \"audioclass[perch]\"\n</code></pre>"},{"location":"reference/models/perch/#audioclass.models.perch","title":"<code>audioclass.models.perch</code>","text":"<p>Module for loading and using the Google Perch audio classification model.</p> <p>This module provides a convenient interface for working with the Perch model, a TensorFlow Hub-based model designed for bird sound classification. It includes the <code>Perch</code> class, which is a subclass of <code>TensorflowModel</code>, and functions for loading the model and its associated labels.</p> Notes <p>The Perch model is hosted on Kaggle. Depending on your network configuration, you might need to set up Kaggle API credentials to access the model. Refer to Kaggle's documentation for instructions.</p> <p>This package is not affiliated with Google Research, the original developers of the Perch model.</p> <p>Classes:</p> Name Description <code>Perch</code> <p>Google Perch audio classification model.</p> <p>Functions:</p> Name Description <code>get_signature</code> <p>Get the signature of a Perch model.</p> <code>load_tags</code> <p>Load Perch labels from a file.</p> <p>Attributes:</p> Name Type Description <code>INPUT_SAMPLES</code> <p>Default number of samples expected in the input tensor.</p> <code>MODEL_PATH</code> <p>Default path to the Perch TensorFlow Hub model URL.</p> <code>SAMPLERATE</code> <p>Default sample rate of the audio data expected by the model (in Hz).</p> <code>TAGS_PATH</code> <p>Default path to the Perch labels file.</p> <code>ebird2021</code> <code>ebird2021_def</code>"},{"location":"reference/models/perch/#audioclass.models.perch-attributes","title":"Attributes","text":""},{"location":"reference/models/perch/#audioclass.models.perch.INPUT_SAMPLES","title":"<code>INPUT_SAMPLES = 160000</code>  <code>module-attribute</code>","text":"<p>Default number of samples expected in the input tensor.</p> <p>This value corresponds to 5 seconds of audio data at a sample rate of 32,000 Hz.</p>"},{"location":"reference/models/perch/#audioclass.models.perch.MODEL_PATH","title":"<code>MODEL_PATH = 'https://www.kaggle.com/models/google/bird-vocalization-classifier/TensorFlow2/bird-vocalization-classifier/4'</code>  <code>module-attribute</code>","text":"<p>Default path to the Perch TensorFlow Hub model URL.</p>"},{"location":"reference/models/perch/#audioclass.models.perch.SAMPLERATE","title":"<code>SAMPLERATE = 32000</code>  <code>module-attribute</code>","text":"<p>Default sample rate of the audio data expected by the model (in Hz).</p>"},{"location":"reference/models/perch/#audioclass.models.perch.TAGS_PATH","title":"<code>TAGS_PATH = DATA_DIR / 'perch' / 'label.csv'</code>  <code>module-attribute</code>","text":"<p>Default path to the Perch labels file.</p>"},{"location":"reference/models/perch/#audioclass.models.perch.ebird2021","title":"<code>ebird2021 = data.Term(uri='https://www.birds.cornell.edu/clementschecklist/wp-content/uploads/2021/08/eBird_Taxonomy_v2021.csv', label='ebird2021', name='ebird:2021speciescodes', definition=ebird2021_def)</code>  <code>module-attribute</code>","text":""},{"location":"reference/models/perch/#audioclass.models.perch.ebird2021_def","title":"<code>ebird2021_def = 'The eBird 2021 taxonomy is a global list of bird species used for reporting sightings in eBird.\\nIt includes all species and subspecies, and is updated annually to reflect the latest ornithological knowledge.\\nThis comprehensive list is used across various Cornell Lab projects and is vital for data analysis,\\nbird identification, and citizen science initiatives.\\n\\nFor more information and to download the taxonomy, visit the eBird website.\\n'</code>  <code>module-attribute</code>","text":""},{"location":"reference/models/perch/#audioclass.models.perch-classes","title":"Classes","text":""},{"location":"reference/models/perch/#audioclass.models.perch.Perch","title":"<code>Perch(callable, signature, tags, confidence_threshold, samplerate, name, logits=True, batch_size=8)</code>","text":"<p>               Bases: <code>TensorflowModel</code></p> <p>Google Perch audio classification model.</p> <p>This class is a wrapper around a TensorFlow Hub model for bird sound classification. It provides methods for loading the model, processing audio data, and returning predictions.</p> <p>Methods:</p> Name Description <code>load</code> <p>Load a Perch model from a URL.</p>"},{"location":"reference/models/perch/#audioclass.models.perch.Perch-functions","title":"Functions","text":""},{"location":"reference/models/perch/#audioclass.models.perch.Perch.load","title":"<code>load(model_url=MODEL_PATH, tags_url=TAGS_PATH, confidence_threshold=DEFAULT_THRESHOLD, samplerate=SAMPLERATE, name='Perch', batch_size=8)</code>  <code>classmethod</code>","text":"<p>Load a Perch model from a URL.</p> <p>Parameters:</p> Name Type Description Default <code>model_url</code> <code>Union[Path, str]</code> <p>The URL of the TensorFlow Hub model. Defaults to the official Perch model URL.</p> <code>MODEL_PATH</code> <code>tags_url</code> <code>Union[Path, str]</code> <p>The URL or path to the file containing the labels. Defaults to the tags file included in the package.</p> <code>TAGS_PATH</code> <code>confidence_threshold</code> <code>float</code> <p>The minimum confidence threshold for making predictions. Defaults to <code>DEFAULT_THRESHOLD</code>.</p> <code>DEFAULT_THRESHOLD</code> <code>samplerate</code> <code>int</code> <p>The sample rate of the audio data expected by the model (in Hz). Defaults to <code>SAMPLERATE</code>.</p> <code>SAMPLERATE</code> <code>name</code> <code>str</code> <p>The name of the model. Defaults to \"Perch\".</p> <code>'Perch'</code> <code>batch_size</code> <code>int</code> <p>The batch size used for processing audio data. Defaults to 8.</p> <code>8</code> <p>Returns:</p> Type Description <code>Perch</code> <p>An instance of the Perch class.</p>"},{"location":"reference/models/perch/#audioclass.models.perch-functions","title":"Functions","text":""},{"location":"reference/models/perch/#audioclass.models.perch.get_signature","title":"<code>get_signature(callable)</code>","text":"<p>Get the signature of a Perch model.</p> <p>Parameters:</p> Name Type Description Default <code>callable</code> <code>Callable</code> <p>The TensorFlow callable representing the model.</p> required <p>Returns:</p> Type Description <code>Signature</code> <p>The signature of the Perch model.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the model does not have exactly one input tensor, if the input tensor does not have 2 dimensions, or if the model does not have exactly two output tensors.</p>"},{"location":"reference/models/perch/#audioclass.models.perch.load_tags","title":"<code>load_tags(path=TAGS_PATH)</code>","text":"<p>Load Perch labels from a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[Path, str]</code> <p>Path or URL to the file containing the labels. Defaults to the tags file included in the package.</p> <code>TAGS_PATH</code> <p>Returns:</p> Type Description <code>List[Tag]</code> <p>List of soundevent <code>Tag</code> objects.</p>"},{"location":"reference/models/tensorflow/","title":"tensorflow","text":"<p>Warning</p> <p>Additional Dependencies: To use the Tensorflow models, you will need to install additional dependencies. These dependencies are optional, as they can be heavy and may not be needed in all use cases. To install them, run:</p> <pre><code>pip install \"audioclass[tensorflow]\"\n</code></pre>"},{"location":"reference/models/tensorflow/#audioclass.models.tensorflow","title":"<code>audioclass.models.tensorflow</code>","text":"<p>Module for defining TensorFlow-based audio classification models.</p> <p>This module provides classes and functions for creating and using TensorFlow models for audio classification tasks. It includes a <code>TensorflowModel</code> class that wraps a TensorFlow callable and a <code>Signature</code> dataclass to define the model's input and output specifications.</p> <p>Classes:</p> Name Description <code>Signature</code> <p>Defines the input and output signature of a TensorFlow model.</p> <code>TensorflowModel</code> <p>A wrapper class for TensorFlow audio classification models.</p>"},{"location":"reference/models/tensorflow/#audioclass.models.tensorflow-classes","title":"Classes","text":""},{"location":"reference/models/tensorflow/#audioclass.models.tensorflow.Signature","title":"<code>Signature(input_name, classification_name, feature_name, input_length, input_dtype=np.float32)</code>  <code>dataclass</code>","text":"<p>Defines the input and output signature of a TensorFlow model.</p> <p>Attributes:</p> Name Type Description <code>classification_name</code> <code>str</code> <p>The name of the output tensor containing classification probabilities.</p> <code>feature_name</code> <code>str</code> <p>The name of the output tensor containing extracted features.</p> <code>input_dtype</code> <code>DTypeLike</code> <p>The data type of the input tensor. Defaults to np.float32.</p> <code>input_length</code> <code>int</code> <p>The number of samples expected in the input tensor.</p> <code>input_name</code> <code>str</code> <p>The name of the input tensor.</p>"},{"location":"reference/models/tensorflow/#audioclass.models.tensorflow.Signature-attributes","title":"Attributes","text":""},{"location":"reference/models/tensorflow/#audioclass.models.tensorflow.Signature.classification_name","title":"<code>classification_name</code>  <code>instance-attribute</code>","text":"<p>The name of the output tensor containing classification probabilities.</p>"},{"location":"reference/models/tensorflow/#audioclass.models.tensorflow.Signature.feature_name","title":"<code>feature_name</code>  <code>instance-attribute</code>","text":"<p>The name of the output tensor containing extracted features.</p>"},{"location":"reference/models/tensorflow/#audioclass.models.tensorflow.Signature.input_dtype","title":"<code>input_dtype = np.float32</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The data type of the input tensor. Defaults to np.float32.</p>"},{"location":"reference/models/tensorflow/#audioclass.models.tensorflow.Signature.input_length","title":"<code>input_length</code>  <code>instance-attribute</code>","text":"<p>The number of samples expected in the input tensor.</p>"},{"location":"reference/models/tensorflow/#audioclass.models.tensorflow.Signature.input_name","title":"<code>input_name</code>  <code>instance-attribute</code>","text":"<p>The name of the input tensor.</p>"},{"location":"reference/models/tensorflow/#audioclass.models.tensorflow.TensorflowModel","title":"<code>TensorflowModel(callable, signature, tags, confidence_threshold, samplerate, name, logits=True, batch_size=8)</code>","text":"<p>               Bases: <code>ClipClassificationModel</code></p> <p>A wrapper class for TensorFlow audio classification models.</p> <p>This class provides a standardized interface for interacting with TensorFlow models, allowing them to be used seamlessly with the audioclass library.</p> <p>Parameters:</p> Name Type Description Default <code>callable</code> <code>Callable</code> <p>The TensorFlow callable representing the model.</p> required <code>signature</code> <code>Signature</code> <p>The input and output signature of the model.</p> required <code>tags</code> <code>List[Tag]</code> <p>The list of tags that the model can predict.</p> required <code>confidence_threshold</code> <code>float</code> <p>The minimum confidence threshold for assigning a tag to a clip.</p> required <code>samplerate</code> <code>int</code> <p>The sample rate of the audio data expected by the model (in Hz).</p> required <code>name</code> <code>str</code> <p>The name of the model.</p> required <code>logits</code> <code>bool</code> <p>Whether the model outputs logits (True) or probabilities (False). Defaults to True.</p> <code>True</code> <code>batch_size</code> <code>int</code> <p>The maximum number of frames to process in each batch. Defaults to 8.</p> <code>8</code> <p>Methods:</p> Name Description <code>process_array</code> <p>Process a single audio array and return the model output.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>callable</code> <code>Callable</code> <p>The TensorFlow callable representing the model.</p> <code>confidence_threshold</code> <code>input_samples</code> <code>logits</code> <code>name</code> <code>num_classes</code> <code>samplerate</code> <code>signature</code> <code>Signature</code> <p>The input and output signature of the model.</p> <code>tags</code>"},{"location":"reference/models/tensorflow/#audioclass.models.tensorflow.TensorflowModel-attributes","title":"Attributes","text":""},{"location":"reference/models/tensorflow/#audioclass.models.tensorflow.TensorflowModel.batch_size","title":"<code>batch_size = batch_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/models/tensorflow/#audioclass.models.tensorflow.TensorflowModel.callable","title":"<code>callable = callable</code>  <code>instance-attribute</code>","text":"<p>The TensorFlow callable representing the model.</p>"},{"location":"reference/models/tensorflow/#audioclass.models.tensorflow.TensorflowModel.confidence_threshold","title":"<code>confidence_threshold = confidence_threshold</code>  <code>instance-attribute</code>","text":""},{"location":"reference/models/tensorflow/#audioclass.models.tensorflow.TensorflowModel.input_samples","title":"<code>input_samples = signature.input_length</code>  <code>instance-attribute</code>","text":""},{"location":"reference/models/tensorflow/#audioclass.models.tensorflow.TensorflowModel.logits","title":"<code>logits = logits</code>  <code>instance-attribute</code>","text":""},{"location":"reference/models/tensorflow/#audioclass.models.tensorflow.TensorflowModel.name","title":"<code>name = name</code>  <code>instance-attribute</code>","text":""},{"location":"reference/models/tensorflow/#audioclass.models.tensorflow.TensorflowModel.num_classes","title":"<code>num_classes = len(tags)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/models/tensorflow/#audioclass.models.tensorflow.TensorflowModel.samplerate","title":"<code>samplerate = samplerate</code>  <code>instance-attribute</code>","text":""},{"location":"reference/models/tensorflow/#audioclass.models.tensorflow.TensorflowModel.signature","title":"<code>signature = signature</code>  <code>instance-attribute</code>","text":"<p>The input and output signature of the model.</p>"},{"location":"reference/models/tensorflow/#audioclass.models.tensorflow.TensorflowModel.tags","title":"<code>tags = tags</code>  <code>instance-attribute</code>","text":""},{"location":"reference/models/tensorflow/#audioclass.models.tensorflow.TensorflowModel-functions","title":"Functions","text":""},{"location":"reference/models/tensorflow/#audioclass.models.tensorflow.TensorflowModel.process_array","title":"<code>process_array(array)</code>","text":"<p>Process a single audio array and return the model output.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>The audio array to be processed, with shape <code>(num_frames, input_samples)</code>.</p> required <p>Returns:</p> Type Description <code>ModelOutput</code> <p>A <code>ModelOutput</code> object containing the class probabilities and extracted features.</p> Note <p>This is a low-level method that requires manual batching of the input audio array. If you prefer a higher-level interface that handles batching automatically, consider using <code>process_file</code>, <code>process_recording</code>, or <code>process_clip</code> instead.</p> <p>Be aware that passing an array with a large batch size may exceed available device memory and cause the process to crash.</p>"},{"location":"reference/models/tensorflow/#audioclass.models.tensorflow-functions","title":"Functions","text":""},{"location":"reference/models/tensorflow/#audioclass.models.tensorflow.process_array","title":"<code>process_array(call, signature, array, validate_signature=False, logits=True)</code>","text":"<p>Process an array with a TensorFlow model.</p> <p>Parameters:</p> Name Type Description Default <code>call</code> <code>Callable</code> <p>The TensorFlow callable representing the model.</p> required <code>signature</code> <code>Signature</code> <p>The input and output signature of the model.</p> required <code>array</code> <code>ndarray</code> <p>The audio array to be processed, with shape (num_frames, input_samples) or (input_samples,).</p> required <code>validate_signature</code> <code>bool</code> <p>Whether to validate the model signature. Defaults to False.</p> <code>False</code> <code>logits</code> <code>bool</code> <p>Whether the model outputs logits (True) or probabilities (False). Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>ModelOutput</code> <p>A <code>ModelOutput</code> object containing the class probabilities and extracted features.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input array has the wrong shape or if the model signature is invalid.</p>"},{"location":"reference/models/tflite/","title":"tflite","text":"<p>Warning</p> <p>Additional Dependencies: To use the Tensorflow Lite models, you will need to install additional dependencies. These dependencies are optional, as they can be heavy and may not be needed in all use cases. To install them, run:</p> <pre><code>pip install \"audioclass[tflite]\"\n</code></pre>"},{"location":"reference/models/tflite/#audioclass.models.tflite","title":"<code>audioclass.models.tflite</code>","text":"<p>Module for defining TensorFlow Lite-based audio classification models.</p> <p>This module provides classes and functions for creating and using TensorFlow Lite models for audio classification tasks. It includes a <code>TFLiteModel</code> class that wraps a TensorFlow Lite interpreter and a <code>Signature</code> dataclass to define the model's input and output specifications.</p> <p>Classes:</p> Name Description <code>Signature</code> <p>Defines the input and output signature of a TensorFlow Lite model.</p> <code>TFLiteModel</code> <p>A wrapper class for TensorFlow Lite audio classification models.</p> <p>Functions:</p> Name Description <code>load_model</code> <p>Load a TensorFlow Lite model from a file.</p>"},{"location":"reference/models/tflite/#audioclass.models.tflite-classes","title":"Classes","text":""},{"location":"reference/models/tflite/#audioclass.models.tflite.Signature","title":"<code>Signature(input_index, classification_index, feature_index, input_length, input_dtype=np.float32)</code>  <code>dataclass</code>","text":"<p>Defines the input and output signature of a TensorFlow Lite model.</p> <p>Attributes:</p> Name Type Description <code>classification_index</code> <code>int</code> <p>The index of the tensor containing classification probabilities.</p> <code>feature_index</code> <code>int</code> <p>The index of the tensor containing extracted features.</p> <code>input_dtype</code> <code>DTypeLike</code> <p>The data type of the input tensor. Defaults to np.float32.</p> <code>input_index</code> <code>int</code> <p>The index of the input tensor in the model.</p> <code>input_length</code> <code>int</code> <p>The number of audio samples expected in the input tensor.</p>"},{"location":"reference/models/tflite/#audioclass.models.tflite.Signature-attributes","title":"Attributes","text":""},{"location":"reference/models/tflite/#audioclass.models.tflite.Signature.classification_index","title":"<code>classification_index</code>  <code>instance-attribute</code>","text":"<p>The index of the tensor containing classification probabilities.</p>"},{"location":"reference/models/tflite/#audioclass.models.tflite.Signature.feature_index","title":"<code>feature_index</code>  <code>instance-attribute</code>","text":"<p>The index of the tensor containing extracted features.</p>"},{"location":"reference/models/tflite/#audioclass.models.tflite.Signature.input_dtype","title":"<code>input_dtype = np.float32</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The data type of the input tensor. Defaults to np.float32.</p>"},{"location":"reference/models/tflite/#audioclass.models.tflite.Signature.input_index","title":"<code>input_index</code>  <code>instance-attribute</code>","text":"<p>The index of the input tensor in the model.</p>"},{"location":"reference/models/tflite/#audioclass.models.tflite.Signature.input_length","title":"<code>input_length</code>  <code>instance-attribute</code>","text":"<p>The number of audio samples expected in the input tensor.</p>"},{"location":"reference/models/tflite/#audioclass.models.tflite.TFLiteModel","title":"<code>TFLiteModel(interpreter, signature, tags, confidence_threshold, samplerate, name, logits=True, batch_size=8)</code>","text":"<p>               Bases: <code>ClipClassificationModel</code></p> <p>A wrapper class for TensorFlow Lite audio classification models.</p> <p>This class provides a standardized interface for interacting with TensorFlow Lite models, allowing them to be used seamlessly with the audioclass library.</p> <p>Parameters:</p> Name Type Description Default <code>interpreter</code> <code>Interpreter</code> <p>The TensorFlow Lite interpreter object.</p> required <code>signature</code> <code>Signature</code> <p>The input and output signature of the model.</p> required <code>tags</code> <code>List[Tag]</code> <p>The list of tags that the model can predict.</p> required <code>confidence_threshold</code> <code>float</code> <p>The minimum confidence threshold for assigning a tag to a clip.</p> required <code>samplerate</code> <code>int</code> <p>The sample rate of the audio data expected by the model (in Hz).</p> required <code>name</code> <code>str</code> <p>The name of the model.</p> required <code>logits</code> <code>bool</code> <p>Whether the model outputs logits (True) or probabilities (False). Defaults to True.</p> <code>True</code> <code>batch_size</code> <code>int</code> <p>The maximum number of frames to process in each batch. Defaults to 8.</p> <code>8</code> <p>Methods:</p> Name Description <code>process_array</code> <p>Process a single audio array and return the model output.</p> <p>Attributes:</p> Name Type Description <code>batch_size</code> <code>confidence_threshold</code> <code>input_samples</code> <code>interpreter</code> <code>Interpreter</code> <p>The TensorFlow Lite interpreter object.</p> <code>logits</code> <code>name</code> <code>num_classes</code> <code>samplerate</code> <code>signature</code> <code>Signature</code> <p>The input and output signature of the model.</p> <code>tags</code>"},{"location":"reference/models/tflite/#audioclass.models.tflite.TFLiteModel-attributes","title":"Attributes","text":""},{"location":"reference/models/tflite/#audioclass.models.tflite.TFLiteModel.batch_size","title":"<code>batch_size = batch_size</code>  <code>instance-attribute</code>","text":""},{"location":"reference/models/tflite/#audioclass.models.tflite.TFLiteModel.confidence_threshold","title":"<code>confidence_threshold = confidence_threshold</code>  <code>instance-attribute</code>","text":""},{"location":"reference/models/tflite/#audioclass.models.tflite.TFLiteModel.input_samples","title":"<code>input_samples = signature.input_length</code>  <code>instance-attribute</code>","text":""},{"location":"reference/models/tflite/#audioclass.models.tflite.TFLiteModel.interpreter","title":"<code>interpreter = interpreter</code>  <code>instance-attribute</code>","text":"<p>The TensorFlow Lite interpreter object.</p>"},{"location":"reference/models/tflite/#audioclass.models.tflite.TFLiteModel.logits","title":"<code>logits = logits</code>  <code>instance-attribute</code>","text":""},{"location":"reference/models/tflite/#audioclass.models.tflite.TFLiteModel.name","title":"<code>name = name</code>  <code>instance-attribute</code>","text":""},{"location":"reference/models/tflite/#audioclass.models.tflite.TFLiteModel.num_classes","title":"<code>num_classes = len(tags)</code>  <code>instance-attribute</code>","text":""},{"location":"reference/models/tflite/#audioclass.models.tflite.TFLiteModel.samplerate","title":"<code>samplerate = samplerate</code>  <code>instance-attribute</code>","text":""},{"location":"reference/models/tflite/#audioclass.models.tflite.TFLiteModel.signature","title":"<code>signature = signature</code>  <code>instance-attribute</code>","text":"<p>The input and output signature of the model.</p>"},{"location":"reference/models/tflite/#audioclass.models.tflite.TFLiteModel.tags","title":"<code>tags = tags</code>  <code>instance-attribute</code>","text":""},{"location":"reference/models/tflite/#audioclass.models.tflite.TFLiteModel-functions","title":"Functions","text":""},{"location":"reference/models/tflite/#audioclass.models.tflite.TFLiteModel.process_array","title":"<code>process_array(array)</code>","text":"<p>Process a single audio array and return the model output.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>ndarray</code> <p>The audio array to be processed, with shape <code>(num_frames, input_samples)</code>.</p> required <p>Returns:</p> Type Description <code>ModelOutput</code> <p>A <code>ModelOutput</code> object containing the class probabilities and extracted features.</p> Note <p>This is a low-level method that requires manual batching of the input audio array. If you prefer a higher-level interface that handles batching automatically, consider using <code>process_file</code>, <code>process_recording</code>, or <code>process_clip</code> instead.</p> <p>Be aware that passing an array with a large batch size may exceed available device memory and cause the process to crash.</p>"},{"location":"reference/models/tflite/#audioclass.models.tflite-functions","title":"Functions","text":""},{"location":"reference/models/tflite/#audioclass.models.tflite.load_model","title":"<code>load_model(path, num_threads=None)</code>","text":"<p>Load a TensorFlow Lite model from a file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>Union[Path, str]</code> <p>The path to the TensorFlow Lite model file.</p> required <code>num_threads</code> <code>Optional[int]</code> <p>The number of threads to use for inference. If None, the default number of threads will be used.</p> <code>None</code> <p>Returns:</p> Type Description <code>Interpreter</code> <p>The TensorFlow Lite interpreter object.</p>"},{"location":"reference/models/tflite/#audioclass.models.tflite.process_array","title":"<code>process_array(interpreter, signature, array, validate_signature=False, logits=True)</code>","text":"<p>Process an array with a TF Lite model.</p> <p>Parameters:</p> Name Type Description Default <code>interpreter</code> <code>Interpreter</code> <p>The TF Lite model interpreter.</p> required <code>signature</code> <code>Signature</code> <p>The input and output signature of the model.</p> required <code>array</code> <code>ndarray</code> <p>The audio array to be processed, with shape (num_frames, input_samples) or (input_samples,).</p> required <code>validate_signature</code> <code>bool</code> <p>Whether to validate the model signature. Defaults to False.</p> <code>False</code> <code>logits</code> <code>bool</code> <p>Whether the model outputs logits (True) or probabilities (False). Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>ModelOutput</code> <p>A <code>ModelOutput</code> object containing the class probabilities and extracted features.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input array has the wrong shape or if the model signature is invalid.</p>"}]}